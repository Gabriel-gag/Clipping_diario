# -*- coding: utf-8 -*-
"""clipping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bhc4IQcSYKlpaBEXUb-uAbgp2uy0OKYZ
"""
from dotenv import load_dotenv
load_dotenv()

# -----------------------
#         IMPORTS
# -----------------------
import os
import re
import json
import time
import email
import jinja2
import imaplib
import smtplib
import requests
import pandas as pd
import logging
import fitz  # PyMuPDF

from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, date
from typing import List, Dict
from pydantic import BaseModel
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from email.mime.multipart import MIMEMultipart

import google.generativeai as genai
from google.generativeai import types

# -----------------------
#      CONFIGURAÇÕES
# -----------------------
GMAIL_USER = os.getenv("GMAIL_USER")
GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD")
SMTP_USER = GMAIL_USER
SMTP_PASSWORD = GMAIL_APP_PASSWORD
MODEL_NAME = 'gemini-2.5-flash'

FILEPATH_INTERESSADOS = 'clipping_interessados_teste.xlsx'
DATA_HOJE = datetime.now().strftime('%d/%m/%Y')

# Se True, filtra para pegar apenas o email do dia
# Caso False, pega o último email do clipping
SOMENTE_HOJE=True

# --- Configuração de logging ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    force=True
)
logger = logging.getLogger(__name__)

# -----------------------
#       MODELO
# -----------------------

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
client = genai.GenerativeModel(MODEL_NAME)

materia_json_schema = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "titulo": {"type": "string"},
            "descricao": {"type": "string"},
            "esfera": {"type": "string"},
            "relevancia": {"type": "string"},
            "url": {"type": "string"},
            "paginas": {"type": "string"},
        },
        "required": ["titulo", "descricao", "esfera", "relevancia", "url", "paginas"]
    }
}

# -----------------------
#         PROMPT
# -----------------------
def get_prompt(setor_sigla:str, setor_descricao: str, setor_interesse: str, article_url: str = None, lista_urls = []) -> str:

    prompt_urls_pdf = ''
    if len(lista_urls):
        prompt_urls_pdf = f"""
        ## Lista de Links encontrados no documento
        A seguir será apresentada uma lista contendo toda as urls relevantes encontradas no documento anexo. Cada elemento da lista é formado pela `url`, `texto` associado a URL e a `pagina` em que foi encontrada. Utilize essa informação para tentar associar uma URL juntamente com a matéria selecionada.

        {json.dumps(lista_urls, indent=2, ensure_ascii=False)}\n
        """


    return f"""
## System Prompt:
Você é um assistente especializado em seleção de matérias jornalísticas que tenham relevância e pertinência com as atribuições da {setor_descricao} - {setor_sigla}.

## Contexto:
{setor_interesse}

## Objetivo:
Extrair, estruturar e analisar o documento fornecido em busca das matérias de interesse da {setor_sigla} com precisão e objetividade.

---
{prompt_urls_pdf}

## Instruções de Análise e Resposta:

1. **Leia o documento (PDF ou conteúdo web) de forma detalhada e abrangente** antes de iniciar a extração de informações.
2. Para cada matéria de interesse selecionada, você deve fornecer a resposta em um **objeto JSON** único.
3. Cada item de matéria deve ser um objeto com as seguintes chaves:
    * `titulo`: String contendo o título da matéria selecionada.
    * `descricao`: String com o resumo completo e claro da matéria.
    * `esfera`: String ou lista de strings classificando a(s) esfera(s) em que trata a matéria e os citados (Federal, Estadual ou Municipal). Se houver mais de uma, liste a principal primeiro.
    * `relevancia`: String explicando por que a matéria é relevante para a atuação da {setor_sigla}.
    * `url`: String contendo a URL da matéria fornecida pelo sistema do clipping (multclipp). Se não houver URL aplicável, retorne nulo `None`.
    * `paginas`: String com o(s) número(s) da(s) página(s) (para PDFs). Se o documento não for paginado ou a informação não estiver disponível, retorne nulo `None`.
4. A resposta final deve ser uma lista desses objetos. Caso não haja nenhuma informação relevante, retorne uma lista vazia: `[]`.

## Exemplo de saída:
[{{
    "titulo": "TCU aponta falhas em política de alfabetização e cobra MEC",
    "descricao": "Uma auditoria do Tribunal de Contas da União (TCU) revelou deficiências na execução do Compromisso Nacional Criança Alfabetizada, principal iniciativa do governo para alfabetizar crianças até o 2º ano do ensino fundamental e recuperar aprendizagens do 3º ao 5º ano. O relatório aponta que 44% dos estudantes do 2º ano da rede pública não estavam alfabetizados em 2023. As falhas incluem baixa articulação entre o MEC e entes subnacionais, comitês criados de forma isolada e sobreposta, e falta de monitoramento adequado. O TCU recomendou ao MEC que avalie a maturidade dos comitês estaduais e direcione ações para qualificar a gestão nos territórios com piores resultados. A meta de alfabetizar 100% das crianças até o final do 2º ano e 80% dos alunos até 2030 é considerada desafiadora.",
    "esfera": "Federal",
    "relevancia": "Esta matéria é altamente relevante para a CAD-Educação, pois trata de uma auditoria do TCU sobre uma política nacional de "alfabetização" e "educação" ("Compromisso Nacional Criança Alfabetizada"). As falhas apontadas na execução do programa, a baixa articulação entre o "MEC" e as "instituições educacionais" locais, e a falta de "monitoramento" são pontos cruciais para a atuação da CAD-Educação na fiscalização da "rede pública de ensino", "alunos", "professores" e "projetos escolares" relacionados à qualidade da educação.",
    "url": "http://alpha.multclipp.com.br/n/WAz81FIfOYne0uYRuuXsaYgzK6GBItrc0",
    "paginas": "153, 154, 155"
  }}]

Fonte de Informação: {article_url or "Documento PDF Anexo"}
"""

# -----------------------
#     PROCESSAMENTO PDF
# -----------------------

def extrair_links_com_texto_pdf(filepath: Path) -> list[dict]:
    """
    Extrai todos os links de um PDF, capturando tanto o texto âncora
    quanto a URL de destino.

    Retorna:
        Uma lista de dicionários, ex: [{'texto': 'O Globo', 'url': 'http://...'}]
    """
    links_com_texto = []

    try:
        with fitz.open(filepath) as doc:
            for page_num, page in enumerate(doc, start=1):
                # Pega todos os objetos de link da página
                links = page.get_links()

                for link in links:
                    # Nos interessa apenas links que apontam para uma URL externa
                    if link.get('kind') == fitz.LINK_URI:

                        # 'from' é o retângulo (fitz.Rect) que define a área do link
                        rect = link['from']

                        # Extrai o texto DENTRO daquele retângulo específico
                        texto_do_link = page.get_text("text", clip=rect).strip()

                        # Pega a URL de destino
                        url_de_destino = link.get('uri', '')

                        # Adiciona o par (texto, url) à nossa lista se o texto não for vazio
                        if texto_do_link and url_de_destino:
                            links_com_texto.append({
                                "pagina": page_num,
                                "texto": texto_do_link,
                                "url": url_de_destino
                            })

    except Exception as e:
        print(f"Erro ao processar PDF com PyMuPDF: {e}")
        return []

    links_nao_amazon = list(filter(lambda d: 'amazonaws' not in d['url'], links_com_texto))
    return links_nao_amazon

def avalia_clipping(uploaded_file, prompt_text: str, modelo=MODEL_NAME):
    logger.info(f"Processando anexo com Gemini...")

    try:
        contents = [prompt_text, uploaded_file] if uploaded_file is not None else [prompt_text]
        response = client.generate_content( # Corrected line: removed .models
            contents=contents,
            generation_config=types.GenerationConfig( # Use generation_config instead of config
                # Adicionado para deixar o modelo mais determinístico (padrão é 1.0)
                temperature=0.2,
                response_mime_type='application/json',
                response_schema=materia_json_schema
            )
        )

        return response.text
    except Exception as e:
        logger.error(f"Erro API Gemini (PDF): {e}")
        return "[]"

# -----------------------
#     PROCESSA LINKS
# -----------------------

def get_content_from_url(url: str) -> str:
    """
    Acessa uma URL, faz web scraping e retorna o texto principal da página.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, timeout=15, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Remover elementos de script, estilo, navegação, rodapé que não são parte do conteúdo principal
        for unwanted_tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'img']):
            unwanted_tag.decompose()

        # Tentar encontrar o conteúdo principal em tags comuns
        content_tags = ['article', 'main', 'div', 'p']
        main_content = None
        for tag_name in content_tags:
            tag = soup.find(tag_name, class_=re.compile(r'content|article|body|post|main|text', re.I))
            if tag:
                main_content = tag
                break

        # Se não encontrou um bloco específico, tenta um corpo mais geral
        if not main_content:
            main_content = soup.find('body') # Fallback para todo o corpo

        if main_content:
            # Extrair texto, removendo múltiplos espaços em branco e linhas vazias
            text = main_content.get_text(separator='\n', strip=True)
            text = re.sub(r'\n\s*\n', '\n\n', text)

            # Limita o tamanho do texto para evitar estouro de tokens no Gemini
            if len(text) > 100000:
                logger.info(f"Conteúdo de {url} truncado de {len(text)} para 100000 caracteres.")
                return text[:100000]
            return text

        return "" # Retorna string vazia se nenhum conteúdo principal for encontrado

    except requests.exceptions.RequestException as e:
        logger.error(f"Erro de requisição ao acessar {url}: {e}")
        return "" # Retorna vazio para que o Gemini não tente processar erro
    except Exception as e:
        logger.error(f"Erro geral ao processar {url}: {e}")
        return ""

def process_articles_with_gemini(urls: List[str], setor_sigla: str, setor_descricao: str, setor_interesse: str, modelo=MODEL_NAME) -> List[Dict]: # Added parameters for get_prompt
    """
    Processa uma lista de URLs, extrai seus conteúdos e envia tudo de uma vez para o Gemini.
    """
    conteudos_unificados = ""
    url_map = []

    for url in set(urls):  # Remove duplicados aqui também, por segurança
        conteudo = get_content_from_url(url)
        if conteudo.strip():
            conteudos_unificados += f"\n\nURL: {url}\n\n{conteudo}\n{'='*80}\n"
            url_map.append(url)

    if not conteudos_unificados.strip():
        logger.info("Nenhum conteúdo válido extraído das URLs fornecidas.")
        return []

    logger.info(f"Enviando {len(url_map)} matérias combinadas para o Gemini...")

    try:
        prompt_text = get_prompt(setor_sigla=setor_sigla, setor_descricao=setor_descricao, setor_interesse=setor_interesse, article_url="Lote de URLs")

        response = client.generate_content( 
            contents=[prompt_text, conteudos_unificados],
            generation_config=types.GenerationConfig( 
                response_mime_type='application/json',
                response_schema=materia_json_schema,
            )
        )

        try:
            materias = json.loads(response.text)
            if isinstance(materias, list):
                # Atribui a lista de URLs à chave 'paginas' para referência
                for materia in materias:
                    materia['paginas'] = ", ".join(url_map)
                return materias
            else:
                logger.warning(f"AVISO: Resposta do Gemini não é uma lista. Resposta: {response.text[:200]}")
                return []
        except json.JSONDecodeError as e:
            logger.error(f"ERRO DE JSON: Gemini retornou JSON inválido. Erro: {e}. Resposta: {response.text[:200]}")
            return []

    except Exception as e:
        logger.error(f"Erro ao chamar a API Gemini para lote de URLs: {e}")
        return []

# -----------------------
#      COLETA INFO EMAIL
# -----------------------

def get_info_email(somente_hoje=True):
    mail = imaplib.IMAP4_SSL("imap.gmail.com")
    mail.login(GMAIL_USER, GMAIL_APP_PASSWORD)
    mail.select("inbox")

    # O formato DD-Mon-YYYY é o padrão exigido pelo protocolo IMAP.
    data_formatada_imap = date.today().strftime("%d-%b-%Y")

    # Busca todos os e-mails do remetente com o assunto "clipping"
    if somente_hoje:
      typ, data = mail.search(None, f'(FROM "augustocba@tcerj.tc.br" SUBJECT "clipping" SINCE "{data_formatada_imap}")')
    else:
      typ, data = mail.search(None, f'(FROM "augustocba@tcerj.tc.br" SUBJECT "clipping")')

    mail_ids = data[0].split()

    if not mail_ids:
        logger.warning("Nenhum e-mail de clipping encontrado.")
        mail.logout()
        return [], None

    latest_id = mail_ids[-1]
    typ, msg_data = mail.fetch(latest_id, "(RFC822)")
    raw_email = msg_data[0][1]
    msg = email.message_from_bytes(raw_email)

    filepath_anexo = None
    for part in msg.walk():
        if part.get_content_maintype() != 'multipart' and part.get_filename() and part.get_filename().lower().endswith(".pdf"):
            os.makedirs("clipping", exist_ok=True)
            filepath_anexo = f"clipping/{part.get_filename()}"
            with open(filepath_anexo, "wb") as f:
                f.write(part.get_payload(decode=True))

    links = []
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_type() in ['text/plain', 'text/html'] and 'attachment' not in str(part.get('Content-Disposition')):
                try:
                    body = part.get_payload(decode=True).decode('latin-1', errors='ignore')
                    found = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', body)
                    links.extend([url for url in found if "https://" in url and len(url) > 20])
                except Exception as e:
                    logger.error(f"Erro ao extrair links: {e}")
                    continue

    mail.logout()
    return list(set(links)), filepath_anexo

# -----------------------
#      E-MAIL FINAL
# -----------------------

def get_template() -> jinja2.Template:
    template_str = """
    <!DOCTYPE html>
    <html lang="pt-BR">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Clipping de Notícias</title>
    </head>
    <body style="font-family: Calibri, sans-serif; margin: 0; padding: 0; background-color: #f4f4f4;">
        <table width="100%" border="0" cellspacing="0" cellpadding="0" style="background-color: #f4f4f4;">
            <tr>
                <td align="center">
                    <table width="800" border="0" cellspacing="0" cellpadding="20" style="background-color: #ffffff; margin-top: 20px; border-radius: 8px; border: 1px solid #ddd;">
                        <tr>
                            <td style="border-bottom: 3px solid #8CC63F; padding-bottom: 15px;">
                                <table width="100%" border="0" cellspacing="0" cellpadding="0">
                                    <tr>
                                        <td width="150" valign="middle">
                                            <img alt="Logo TCE-RJ" width="120" src="https://www.tcerj.tc.br/cdn-storage/logos/logo-horizontal-colorida-para_fundo_branco@3x.png" />
                                        </td>
                                        <td valign="middle" style="font-family: 'Arial Narrow', sans-serif; color: #005A8D; text-align: right;">
                                            <h4 style="margin: 0; font-size: 16px; font-weight: bold;">TRIBUNAL DE CONTAS DO ESTADO DO RIO DE JANEIRO</h4>
                                            <p style="margin: 5px 0 0 0; font-size: 14px;">SECRETARIA GERAL DE CONTROLE EXTERNO</p>
                                            <p style="margin: 5px 0 0 0; font-size: 14px;">SUBSECRETARIA DE CONTROLE DE POLÍTICAS DE CIDADANIA</p>
                                            <p style="margin: 5px 0 0 0; font-size: 14px;">COORDENADORIA DE AUDITORIA DE POLÍTICAS EM TECNOLOGIA DA INFORMAÇÃO</p>
                                        </td>
                                    </tr>
                                </table>
                            </td>
                        </tr>

                        <tr>
                            <td style="padding-top: 30px; padding-bottom: 30px;">
                                <p style="font-size: 16px; color: #333333;">
                                    Prezada {{ setor_sigla }},
                                </p>
                                <p style="font-size: 16px; color: #333333;">
                                    Encaminhamos abaixo a seleção de matérias extraídas do clipping de hoje ({{ data_hoje }}).
                                </p>

                                <hr style="border: 0; border-top: 1px solid #eeeeee; margin-top: 25px;">

                                {% for materia in materias %}
                                <div style="margin-top: 25px;">
                                    <h3 style="color: #0d2548; font-size: 18px; margin-bottom: 10px;">{{ materia.titulo }}</h3>
                                    <p style="font-size: 15px; color: #555555; margin-bottom: 15px; text-align: justify;">
                                        {{ materia.descricao }}
                                    </p>
                                    <p style="font-size: 14px; color: #777777; margin-bottom: 15px; text-align: justify;">
                                        <strong>Esfera:</strong> {{ materia.esfera }}
                                    </p>
                                    <p style="font-size: 14px; color: #777777; margin-bottom: 15px; text-align: justify;">
                                        <strong>Relevância:</strong> {{ materia.relevancia }}
                                    </p>
                                    <p style="font-size: 14px; color: #777777; margin-bottom: 15px; ">
                                        <strong>Página(s):</strong> {{ materia.paginas }}
                                    </p>
                                    {% if materia.url %}
                                    <p style="font-size: 14px; color: #777777; margin: 0;">
                                        <strong>URL:</strong> <a href="{{ materia.url }}">Acesse a matéria</a>
                                    </p>
                                    {% endif %}
                                </div>
                                {% if not loop.last %}
                                <hr style="border: 0; border-top: 1px solid #eeeeee; margin: 25px 0;">
                                {% endif %}
                                {% endfor %}
                            </td>
                        </tr>

                        <tr>
                            <td style="padding: 15px; text-align: center; font-size: 12px; color: #999999; border-top: 1px solid #eeeeee;">
                                <p>Este é um e-mail gerado automaticamente.</p>
                            </td>
                        </tr>
                    </table>
                </td>
            </tr>
        </table>
    </body>
    </html>
    """
    env = jinja2.Environment(loader=jinja2.BaseLoader())
    return env.from_string(template_str)

def gerar_email(materias: List[dict], setor_sigla: str, setor_descricao: str) -> str:
    data_hoje = datetime.now().strftime('%d/%m/%Y')
    template = get_template()

    return template.render(
        data_hoje=data_hoje,
        materias=materias,
        setor_sigla=setor_sigla,
        setor_descricao=setor_descricao.upper()
    )

def enviar_email(destinatarios, assunto, corpo, remetente, senha, caminho_pdf):
    msg = MIMEMultipart()
    msg['Subject'] = assunto
    msg['From'] = remetente
    msg['To'] = ", ".join(destinatarios)

    msg.attach(MIMEText(corpo, 'HTML', 'utf-8'))

    # Anexar o PDF, se um caminho for fornecido
    if caminho_pdf and os.path.exists(caminho_pdf):
        try:
            with open(caminho_pdf, "rb") as fil:
                part_pdf = MIMEApplication(fil.read(),Name=os.path.basename(caminho_pdf))

            # Adicionar cabeçalho para o anexo
            part_pdf['Content-Disposition'] = f'attachment; filename="{os.path.basename(caminho_pdf)}"'

            msg.attach(part_pdf) # Anexar diretamente à mensagem 'mixed' principal
            logger.info(f"Anexando PDF: {os.path.basename(caminho_pdf)}")
        except Exception as e:
            logger.error(f"Erro ao tentar anexar o PDF '{caminho_pdf}': {e}")

    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:
        smtp.login(remetente, senha)
        smtp.sendmail(remetente, destinatarios, msg.as_string())

# -----------------------
#           MAIN
# -----------------------
def main():
        try:
            df_clients = pd.read_excel(FILEPATH_INTERESSADOS)
        except FileNotFoundError:
            logger.error(f"Erro: O arquivo {FILEPATH_INTERESSADOS} não foi encontrado. Por favor, crie-o com as colunas necessárias.")
            return
        
        RECIPIENT_MAPPING = {}
        recipient_mapping_str = os.getenv("RECIPIENT_MAPPING")
        if recipient_mapping_str:
            try:
                RECIPIENT_MAPPING = json.loads(recipient_mapping_str)
                logger.info("Mapeamento de destinatários carregado com sucesso do secret.")
            except json.JSONDecodeError as e:
                logger.error(f"Erro ao decodificar RECIPIENT_MAPPING do secret: {e}. Certifique-se de que é um JSON válido.")
                # Continuar com RECIPIENT_MAPPING vazio
        else:
            logger.warning("RECIPIENT_MAPPING secret não encontrado ou vazio. Verifique a configuração dos secrets no GitHub.")

        logger.info(f"Acessando {GMAIL_USER} para captura das informações.")

        # Coleta PDF anexo e links no corpo do email
        urls, caminho_pdf = get_info_email(somente_hoje=SOMENTE_HOJE)

        # Ja trata o anexo, caso exista
        def extrair_texto_pdf(path_pdf):
            texto = ""
            try:
                with fitz.open(path_pdf) as doc:
                    for page in doc:
                        texto += page.get_text()
            except Exception as e:
                logger.error(f"Erro ao extrair texto do PDF: {e}")
            return texto[:100000]
        
        lista_urls_pdf = extrair_links_com_texto_pdf(caminho_pdf) if caminho_pdf is not None else []

        for index, row in df_clients.iterrows():
            setor_sigla = row['setor_sigla']
            setor_descricao = row['setor_descricao']
            setor_interesse = row['setor_interesse']
            destinatarios = RECIPIENT_MAPPING.get(setor_sigla, [])
            if not destinatarios:
                logger.warning(f"Nenhum destinatário configurado para a sigla '{setor_sigla}' no secret RECIPIENT_MAPPING. Pulando envio para este setor.")
                continue # 

            logger.info('')
            logger.info(f"Processando cliente: {setor_descricao} ({setor_sigla}).")
            logger.info('')

            materias_interessado = []
            if caminho_pdf:
                texto_pdf = extrair_texto_pdf(caminho_pdf)
                prompt = get_prompt(setor_sigla, setor_descricao, setor_interesse, lista_urls=lista_urls_pdf)
                # Avalia PDF Anexo
                response = avalia_clipping(texto_pdf,prompt)
                materias_interessado.extend(json.loads(response))

            # Avalia Links do corpo do email
            if urls:
                logger.info(f"Processando {len(urls)} URLs...")
                materias = process_articles_with_gemini(urls, setor_sigla, setor_descricao, setor_interesse)
                materias_interessado.extend(materias)

            seen = set()
            unicos = []
            for m in materias_interessado:
                if m['titulo'] not in seen:
                    seen.add(m['titulo'])
                    unicos.append(m)

            logger.info(f"Foram selecionadas {len(unicos)} matérias das fontes de informação avaliadas.")

            if unicos:
                corpo = gerar_email(unicos, setor_sigla, setor_descricao)
                enviar_email(
                    destinatarios=destinatarios,
                    assunto=f"[Clipping] {setor_sigla} - {DATA_HOJE}",
                    corpo=corpo,
                    remetente=SMTP_USER,
                    senha=SMTP_PASSWORD,
                    caminho_pdf=caminho_pdf
                )
                logger.info(f"E-mail para {setor_sigla} enviado com sucesso para {', '.join(destinatarios)}!")
            else:
                logger.info(f"Nenhuma matéria relevante encontrada para {setor_sigla}.")

if __name__ == "__main__":
    main()